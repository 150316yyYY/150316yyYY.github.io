<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>Hexo - </title>
    
      <link rel="icon" href="/img/favicon.ico">
    
    <meta name="keyword"  content="">
    
<link rel="stylesheet" href="/css/style.css">

    
      
<link rel="stylesheet" href="/css/helpers.css">
    
    
  
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  <div class="loading-wrapper" data-loading="true">
    <div class="loading">
      <span></span>
      <span></span>
      <span></span>
    </div>
  </div>
  <div class="page" data-filter="true">
    <div class="head" data-show="true">
      <header class="head-header">
  <div class="head-author">
    <a href="/" class="head-author-link">
      Hexo</a>
    </div>
  <div class="head-right">
    <!-- <div class="head-search">
      <input class="head-searchK"></input>
      <span class="head-searchT">
        Search</span>
    </div> -->
    <button class="bar-wrap" id="bar-wrap-toggle">
      <span class="bar"></span>
      <span class="bar"></span>
      <span class="bar"></span>
    </button>
    <div class="head-about" id="head-about">
      
      <a class="head-about-link" href="/about">
        关于</a>
      
    </div>
  </div>
</header>

    </div>
    <div class="main">
      

<div class="menu-bar-head" id="menu-bar" data-show="false">
  <ul class="menu-bar-ul">
    
      
      <li class="menu-bar-item ">
          
              <a href="/categories/Posts/">
          
              <span>Posts1</span>
            </a>
      </li>
    
      
      <li class="menu-bar-item ">
          
              <a href="/categories/Posts2/">
          
              <span>Posts2</span>
            </a>
      </li>
    
      
      <li class="menu-bar-item  border ">
          
            <a href="/archives">
          
              <span>Archives</span>
            </a>
      </li>
    
      
      <li class="menu-bar-item ">
          
            <a href="/tags">
          
              <span>Tags</span>
            </a>
      </li>
    
    
      <li class="menu-bar-item">
        <a href="/about">
          <span>关于</span>
        </a>
      </li>
    
  </ul>
</div>
      <article class="post" id="post">
  <header class="post-head">
    <h1 class="post-title">
      <a class="title" href="/2023/06/19/sqoop/">
        “sqoop”
      </a>
    </h1>
  </header>
  <div class="post-datetag">
    <div class="post-date">
      <time class="post-time" title="2023-06-19 08:42:23" datetime="2023-06-19T00:42:23.000Z" itemprop="datePublished">
  2023-06-19</time>
    </div>
    |
    <div class="post-tag">
      
    </div>
    |
    
  

    <div class="post-visit">
      <span id="busuanzi_container_page_pv">
        <span id="busuanzi_value_page_pv"></span>
        访问
      </span>
    </div>

  


  </div>

  
    <div class="post-word-count">
      

本文共4808字。

    </div>
  

  
    <div class="post-cc">
      版权声明：
      
        署名-非商业性使用-相同方式共享 | <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/2.5/cn/">CC BY-NC-SA 2.5 CN</a>
      
    </div>
  

  


  <div class="article-entry" itemprop="articleBody">
    <span id="more"></span>
<p>第一步 sqoop介绍<br>1.Apache Sqoop是在Hadoop生态体系和RDBMS体系“之间”传送数据的一种工具。来自于Apache软件基金会提供。**<br>2.Sqoop工作机制是将导入或导出命令**<code>翻译成mapreduce程序</code>**来实现。在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。<br>Hadoop生态系统包括：HDFS、Hive、Hbase等<br>RDBMS体系包括：Mysql、Oracle、DB2等<br>Sqoop可以理解为：“SQL 到 Hadoop 和 Hadoop 到SQL”。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%871.png" alt="img"><br>站在Apache立场看待数据流转问题，可以分为数据的导入导出:<br>Import：数据导入。RDBMS—–&gt;Hadoop<br>Export：数据导出。Hadoop—-&gt;RDBMS<br>第二步 sqoop安装<br>1.安装sqoop的前提是已经具备java、mysql、hadoop和hive环境。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%872.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%873.png" alt="img"><br>2.修改配置文件<br>最新稳定版： 1.4.6<br>安装位置：node2<br>配置文件修改：<br>vim &#x2F;etc&#x2F;profile</p>
<p>#SQOOP_HOME<br>export SQOOP_HOME&#x3D;&#x2F;export&#x2F;server&#x2F;sqoop<br>export PATH&#x3D;$PATH:$SQOOP_HOME&#x2F;bin<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%874.png" alt="img"><br>cd $SQOOP_HOME&#x2F;conf<br>mv sqoop-env-template.sh sqoop-env.sh<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%875.png" alt="img"><br>vi sqoop-env.sh<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%876.png" alt="img"><br>export HADOOP_COMMON_HOME&#x3D; &#x2F;export&#x2F;server&#x2F;hadoop<br>export HADOOP_MAPRED_HOME&#x3D; &#x2F;export&#x2F;server&#x2F;hadoop<br>export HIVE_HOME&#x3D; &#x2F;export&#x2F;server&#x2F;hive<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%877.png" alt="img"><br>3.加入mysql的jdbc驱动包<br>cp &#x2F;export&#x2F;server&#x2F;hive&#x2F;lib&#x2F;mysql-connector-java-5.1.32.jar $SQOOP_HOME&#x2F;lib&#x2F;<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%878.png" alt="img"><br>4.验证启动<br>bin&#x2F;sqoop list-databases <br> –connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F; <br> –username root –password hadoop<br>本命令会列出所有mysql的数据库。<br>到这里，整个Sqoop安装工作完成。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%879.png" alt="img"><br>第三步 sqoop导入<br>1.“导入工具”<code>导入单个表从RDBMS到HDFS</code>。表中的<code>每一行被视为HDFS的记录</code>。所有记录都存储为文本文件的文本数据<br>下面的语法用于将数据导入HDFS:<br>$ sqoop import (generic-args) (import-args)<br>2.Sqoop测试表<br>在mysql中创建数据库userdb，然后执行参考资料中的sql脚本：<br>创建三张表: emp雇员表、 emp_add雇员地址表、emp_conn雇员联系表。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8710.png" alt="img"><br>必要补充：在这里浅聊一下有关Navicat Premium 15的安装与使用。<br>1)首先通过下载与老师所给的“Windows必备软件”数据包，在其中的数据库类别下的安装包中，找到名为“Navicat Keygen Patch v5.6.0 DFoX.exe”的安装包，进行下载安装！<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8711.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8712.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8713.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8714.png" alt="img"><br>2)下载相关破解网页<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8715.png" alt="img"><br>3)进入破解网页<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8716.png" alt="img"><br>1.Navicat Premium 15安装教程<br>1)下载后是一个zip压缩包，首先解压到自己想要安装的目录下。<br>2)按照步骤一步一步的安装，这个比较简单。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8717.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8718.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8719.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8720.png" alt="img"><br>3)Navicat Premium 15 激活  （开始激活（激活时必须断网））</p>
<p>4)Patch按钮，选择Navicat的安装位置中的navicat.exe文件<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8721.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8722.png" alt="img"><br>5)点击Generate按钮就会生成一个许可证秘钥，将许可证秘钥复制后就打开Navicat Premium 15<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8723.png" alt="img"><br>6)打开Navicat Premium 15，点击注册<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8724.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8725.png" alt="img"><br>7)弹出的界面选择手动激活<br> <img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8726.png" alt="img"><br>8) 将请求码粘贴到注册机Request Code框中<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8727.png" alt="img"><br>9)点击激活页面的激活弹出框，表示激活成功<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8728.png" alt="img"><br>10)连接MySQL数据库<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8729.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8729.png" alt="img"><br>3.全量导入mysql表数据到HDFS<br>1)下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。<br>#example1-mysql-hdfs-start<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–delete-target-dir <br>–target-dir &#x2F;sqoop&#x2F;sqoopresult <br>–table emp –m 1<br>2)其中–target-dir可以用来指定导出数据存放至HDFS的目录；<br>3)为了验证在HDFS导入的数据，请使用以下命令查看导入的数据：<br>hdfs dfs -cat &#x2F;sqoopresult&#x2F;part-m-00000<br>可以看出它会在HDFS上默认用逗号,分隔emp表的数据和字段。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8730.png" alt="img"><br> 注意：</p>
<ul>
<li>mysql的地址尽量不要使用localhost  请使用ip或者host</li>
<li>如果不指定，导入到hdfs默认分隔符是  “,”</li>
<li>可以通过– fields-terminated-by ‘\t’指定具体的分隔符</li>
<li>如果表的数据比较大 可以并行启动多个maptask执行导入操作，如果表没有主键，请指定根据哪个字段进行切分（使用–m 指定并行度）</li>
</ul>
<p>#example2-mysql-hdfs-terminated<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–target-dir &#x2F;sqoop&#x2F;sqoopresult2 <br>–fields-terminated-by ‘\t’ <br>–table emp –m 1<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8731.png" alt="img"><br>#example3-mysql-hdfs-split<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–target-dir &#x2F;sqoop&#x2F;sqoopresult3 <br>–fields-terminated-by ‘\t’ <br>–split-by id <br>–table emp –m 2<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8732.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8733.png" alt="img"><br>4.全量导入mysql表数据到HIVE<br>1)方式一：先复制表结构到hive中再导入数据<br>①　在hive中新建数据库sqoop_test用于测试<br>create database if not exists sqoop_test comment “this is sqoop db” with dbproperties(‘createdBy’&#x3D;’yzl’);<br>use sqoop_test;<br>show tables;<br>desc formatted emp_add_sp;<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8734.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8735.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8736.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8737.png" alt="img"><br>2)将关系型数据的表结构复制到hive中<br>#example4-1-mysql-hive-structure<br>bin&#x2F;sqoop create-hive-table <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–table emp_add <br>–username root <br>–password hadoop <br>–hive-table sqoop_test.emp_add_sp<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8738.png" alt="img"><br>其中：<br> –table emp_add为mysql中的数据库userdb中的表。<br> –hive-table emp_add_sp 为hive中新建的表名称。<br>3)从关系数据库导入文件到hive中<br>#example4-2-mysql-hive-data<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table emp_add <br>–hive-table sqoop_test.emp_add_sp <br>–hive-import <br>–m 1<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8739.png" alt="img"><br>5.方式二：直接复制表结构数据到hive中<br>#example5-mysql-hive<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table emp_conn <br>–hive-import <br>–m 1 <br>–hive-database sqoop_test;<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8740.png" alt="img"><br>6.导入表数据子集(where过滤)<br>1)–where可以指定从关系数据库导入数据时的查询条件。它执行在数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。<br>#example6-mysql-hdfs-where<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–where “city &#x3D;’sec-bad’” <br>–target-dir &#x2F;sqoop&#x2F;wherequery <br>–table emp_add –m 1<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8741.png" alt="img"><br>7. 导入表数据子集(query查询)<br>1)注意事项：<br>-使用query sql语句来进行查找不能加参数–table ;<br>-并且必须要添加where条件;<br>-并且where条件后面必须带一个$CONDITIONS 这个字符串;<br>-并且这个sql语句必须用单引号，不能用双引号;<br>#example7-mysql-hdfs-query<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–target-dir &#x2F;sqoop&#x2F;wherequery2 <br>–query ‘select id,name,deg from emp WHERE  id&gt;1203 and $CONDITIONS’ <br>–split-by id <br>–fields-terminated-by ‘\001’ <br>–m 2<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8742.png" alt="img"><br>1)sqoop命令中 –split-by id通常配合-m 10参数使用。<br>2)首先sqoop会向关系型数据库比如mysql发送一个命令:select max(id),min(id) from test。<br>3)然后会把max、min之间的区间平均分为10分，最后10个并行的map去找数据库，导数据就正式开始。<br>8.增量导入<br>1)在实际工作当中，数据的导入，很多时候都是只需要导入增量数据即可，并不需要将表中的数据每次都全部导入到hive或者hdfs当中去，这样会造成数据重复的问题。因此一般都是选用一些字段进行增量的导入， sqoop支持增量的导入数据。<br>2)增量导入是仅导入新添加的表中的行的技术。<br>–check-column (col)<br>用来指定一些列，这些列在增量导入时用来检查这些数据是否作为增量数据进行导入，和关系型数据库中的自增字段及时间戳类似。<br>注意:这些被指定的列的类型不能使任意字符类型，如char、varchar等类型都是不可以的，同时– check-column可以去指定多个列。<br>–incremental (mode)<br>append：追加，比如对大于last-value指定的值之后的记录进行追加导入。<br>lastmodified：最后的修改时间，追加last-value指定的日期之后的记录<br>–last-value (value)<br>指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值。<br>9.Append模式增量导入<br>1)执行以下指令先将我们之前的数据导入<br>#example8-1-mysql-hdfs-append<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–target-dir &#x2F;sqoop&#x2F;appendresult <br>–table emp –m 1<br>2)使用hdfs dfs -cat查看生成的数据文件，发现数据已经导入到hdfs中<br>3)然后在mysql的emp表中插入2条数据:<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8743.png" alt="img"><br>4)执行如下的指令，实现增量的导入:<br>#example8-2-mysql-hdfs-append<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table emp –m 1 <br>–target-dir &#x2F;sqoop&#x2F;appendresult <br>–incremental append <br>–check-column id <br>–last-value 1205<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8744.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8745.png" alt="img"><br>5)总结：增量数据的导入</p>
<ul>
<li>所谓的增量数据指的是上次至今中间新增加的数据</li>
<li>sqoop支持两种模式的增量导入 </li>
<li>append追加 根据数值类型字段进行追加导入  大于指定的last-value</li>
<li>lastmodified 根据时间戳类型字段进行追加  大于等于指定的last-value</li>
<li>注意在lastmodified 模式下 还分为两种情形：append  merge-key<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8746.png" alt="img"><br>6)最后验证导入数据目录 可以发现多了一个文件 里面就是增量数据<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8747.png" alt="img"></li>
</ul>
<ol start="10">
<li>Lastmodified模式增量导入<br>1)首先创建一个customer表，指定一个时间戳字段：<br>create table customertest(id int,name varchar(20),last_mod timestamp default current_timestamp on update current_timestamp);<br>此处的时间戳设置为在数据的产生和更新时都会发生改变.<br>2)插入如下记录:<br>insert into customertest(id,name) values(1,’neil’);<br>insert into customertest(id,name) values(2,’jack’);<br>insert into customertest(id,name) values(3,’martin’);<br>insert into customertest(id,name) values(4,’tony’);<br>insert into customertest(id,name) values(5,’eric’);<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8747.png" alt="img"><br>3)此时执行sqoop指令将数据导入hdfs:</li>
</ol>
<p>#example9-1-mysql-hdfs-Lastmodified<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–target-dir &#x2F;sqoop&#x2F;lastmodifiedresult <br>–table customertest –m 1<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8748.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8749.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8750.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8751.png" alt="img"><br>4)查看此时导入的结果数据：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8752.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8753.png" alt="img"><br>5)再次插入一条数据进入customertest表<br>insert into customertest(id,name) values(6,’james’)<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8754.png" alt="img"><br>6)使用incremental的方式进行增量的导入:<br>#example9-2-mysql-hdfs-Lastmodified<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table customertest <br>–target-dir &#x2F;sqoop&#x2F;lastmodifiedresult <br>–check-column last_mod <br>–incremental lastmodified <br>–last-value “2019-05-28 18:42:06” <br>–m 1 <br>–append<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8755.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8756.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8757.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8758.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8759.png" alt="img"><br>7)此处已经会导入我们最后插入的一条记录,但是我们却发现此处插入了2条数据，这是为什么呢？<br>8)这是因为采用**lastmodified模式去处理增量时，会将大于等于last-value值的数据当做增量插入。<br>11.Lastmodified模式:append、merge-key<br>1)使用lastmodified模式进行增量处理要指定增量数据是以<br>append模式(附加)<br>merge-key(合并)模式添加<br>2)下面演示使用merge-by的模式进行增量更新<br>我们去更新 id为1的name字段。<br>update customertest set name &#x3D; ‘Neil’ where id &#x3D; 1;<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8760.png" alt="img"><br>更新之后，这条数据的时间戳会更新为更新数据时的系统时间.<br>3)执行如下指令，把id字段作为merge-key:<br>#example10-mysql-hdfs-merge-key<br>bin&#x2F;sqoop import <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table customertest <br>–target-dir &#x2F;sqoop&#x2F;lastmodifiedresult <br>–check-column last_mod <br>–incremental lastmodified <br>–last-value “2019-05-28 18:42:06” <br>–m 1 <br>–merge-key id<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8761.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8762.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8763.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8764.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8765.png" alt="img"><br>4)由于merge-key这种模式是进行了一次完整的mapreduce操作，<br>5)因此最终我们在lastmodifiedresult文件夹下可以看到生成的为part-r-00000这样的文件，<br>6)会发现id&#x3D;1的name已经得到修改，同时新增了id&#x3D;6的数据<br>总结：<br>7)关于lastmodified 中的两种模式：<br>8)- append 只会追加增量数据到一个新的文件中  并且会产生数据的重复问题<br>9)  因为默认是从指定的last-value 大于等于其值的数据开始导入<br>10)- merge-key 把增量的数据合并到一个文件中  处理追加增量数据之外 如果之前的数据有变化修改<br>11)  也可以进行修改操作 底层相当于进行了一次完整的mr作业。数据不会重复。<br>第四步 sqoop导出<br>将数据从Hadoop生态体系导出到RDBMS数据库导出前，目标表必须存在于目标数据库中。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8766.png" alt="img"><br>export有三种模式：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8767.png" alt="img"></p>
<ol>
<li>默认操作是从将文件中的数据使用INSERT语句插入到表中。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8768.png" alt="img"></li>
<li>更新模式：Sqoop将生成UPDATE替换数据库中现有记录的语句。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8769.png" alt="img"></li>
<li>调用模式：Sqoop将为每条记录创建一个存储过程调用。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8770.png" alt="img"><br>以下是export命令语法：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8771.png" alt="img"><br>$ sqoop export (generic-args) (export-args)<br>1.默认模式导出HDFS数据到mysql<br>默认情况下，sqoop export将每行输入记录转换成一条INSERT语句，添加到目标数据库表中。如果数据库中的表具有约束条件（例如，其值必须唯一的主键列）并且已有数据存在，则必须注意避免插入违反这些约束条件的记录。如果INSERT语句失败，导出过程将失败。此模式主要用于将记录导出到可以接收这些结果的空表中。通常用于全表数据导出。<br>导出时可以是将Hive表中的全部记录或者HDFS数据（可以是全部字段也可以部分字段）导出到Mysql目标表。<br>1)准备HDFS数据<br> 在HDFS文件系统中“&#x2F;emp&#x2F;”目录的下创建一个文件emp_data.txt：<br>mkdir &#x2F;export&#x2F;data&#x2F;sqoop-data&#x2F;emp&#x2F;<br>vim emp_data.txt<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8772.png" alt="img"></li>
</ol>
<p>#上传至hdfs<br>hadoop fs -mkdir &#x2F;sqoop&#x2F;emp_data<br>hadoop fs -put emp_data.txt &#x2F;sqoop&#x2F;emp_data<br>2)手动创建mysql中的目标表<br>mysql&gt; use userdb;<br>mysql&gt; create table employee (<br>   id int not null primary key,<br>   name varchar(20),<br>   deg varchar(20),<br>   salary int,<br>   dept varchar(10));<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8773.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8774.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8775.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8776.png" alt="img"><br>3)执行导出命令<br>#example10-hdfs-mysql-export<br>bin&#x2F;sqoop export <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table employee1 <br>–columns id,name,deg,salary,dept <br>–export-dir &#x2F;sqoop&#x2F;emp_data&#x2F;<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8777.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8778.png" alt="img"><br>4)相关配置参数<br>–input-fields-terminated-by ‘\t’<br>指定文件中的分隔符<br>–columns<br>选择列并控制它们的排序。当导出数据文件和目标表字段列顺序完全一致的时候可以不写。否则以逗号为间隔选择和排列各个列。没有被包含在–columns后面列名或字段要么具备默认值，要么就允许插入空值。否则数据库会拒绝接受sqoop导出的数据，导致Sqoop作业失败<br>–export-dir 导出目录，在执行导出的时候，必须指定这个参数，同时需要具备–table或–call参数两者之一，<br>–table是指的导出数据库当中对应的表，<br>–call是指的某个存储过程。<br>–input-null-string –input-null-non-string<br>如果没有指定第一个参数，对于字符串类型的列来说，“NULL”这个字符串就回被翻译成空值，如果没有使用第二个参数，无论是“NULL”字符串还是说空字符串也好，对于非字符串类型的字段来说，这两个类型的空串都会被翻译成空值。比如：<br>–input-null-string “\N” –input-null-non-string “\N”<br>5)注意<br>数据导出操作<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8779.png" alt="img"></p>
<ul>
<li>注意：导出的目标表需要自己手动提前创建 也就是sqoop并不会帮我们创建复制表结构</li>
<li>导出有三种模式：<ul>
<li>默认模式   目标表是空表  底层把数据一条条insert进去</li>
<li>更新模式   底层是update语句</li>
<li>调用模式   调用存储过程</li>
</ul>
</li>
<li>相关配置参数<ul>
<li>导出文件的分隔符  如果不指定 默认以“,”去切割读取数据文件   –input-fields-terminated-by</li>
<li>如果文件的字段顺序和表中顺序不一致 需要–columns 指定 多个字段之间以”,”</li>
<li>导出的时候需要指定导出数据的目的 export-dir 和导出到目标的表名或者存储过程名</li>
<li>针对空字符串类型和非字符串类型的转换  “\n”<br>2.更新导出（updateonly模式）<br>1)参数说明<br>– update-key,更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8780.png" alt="img"><br>– updatemod，指定updateonly（默认模式），仅仅更新已存在的数据记录，不会插入新纪录。<br>2)准备HDFS数据<br>在HDFS文件系统中&#x2F;sqoop&#x2F;updateonly_1&#x2F;目录的下创建一个文件updateonly_1.txt：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8781.png" alt="img"><br>3)手动创建mysql中的目标表<br>mysql&gt; USE userdb;<br>mysql&gt; CREATE TABLE updateonly (<br> id INT NOT NULL PRIMARY KEY,<br> name VARCHAR(20),<br> deg VARCHAR(20),<br> salary INT);<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8782.png" alt="img"><br>4)先执行全部导出操作</li>
</ul>
</li>
</ul>
<p>#example11-1-hdfs-mysql-export-updateonly<br>bin&#x2F;sqoop export <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table updateonly <br>–export-dir &#x2F;sqoop&#x2F;updateonly_1&#x2F;<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8783.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8784.png" alt="img"><br>5)查看此时mysql中的数据<br> 可以发现是全量导出，全部的数据<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8785.png" alt="img"><br>6) 新增一个文件<br>新增一个文件updateonly_2.txt：修改了前三条数据并且新增了一条记录<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8786.png" alt="img"><br>7) 执行更新导出<br>#example11-2-hdfs-mysql-export-updateonly<br>bin&#x2F;sqoop export <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table updateonly <br>–export-dir &#x2F;sqoop&#x2F;updateonly_2 <br>–update-key id <br>–update-mode updateonly<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8787.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8788.png" alt="img"><br>8)查看最终结果<br>虽然导出时候的日志显示导出4条记录：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8789.png" alt="img"><br>3.更新导出（allowinsert模式）<br>1)参数说明<br>– update-key，更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8790.png" alt="img"><br>– updatemod，指定allowinsert，更新已存在的数据记录，同时插入新纪录。实质上是一个insert &amp; update的操作。<br>2)准备HDFS数据<br>在HDFS &#x2F;sqoop&#x2F;allowinsert_1&#x2F;目录的下创建一个文件allowinsert_1.txt：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8791.png" alt="img"><br>3)手动创建mysql中的目标表<br>mysql&gt; USE userdb;<br>mysql&gt; CREATE TABLE allowinsert (<br>   id INT NOT NULL PRIMARY KEY,<br>   name VARCHAR(20),<br>   deg VARCHAR(20),<br>   salary INT);<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8792.png" alt="img"><br>4)先执行全部导出操作<br>#example12-1-hdfs-mysql-export-allowinsert<br>bin&#x2F;sqoop export <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–table allowinsert <br>–export-dir &#x2F;sqoop&#x2F;allowinsert_1&#x2F;<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8793.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8794.png" alt="img"><br>5)查看此时mysql中的数据<br>6)<br>7)新增文件<br>创建文件allowinsert_2.txt。修改前三条数据并且新增了一条记录。上传至 &#x2F;sqoop&#x2F;allowinsert_2&#x2F;目录下：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8795.png" alt="img"><br>8)执行更新导出<br>#example12-2-hdfs-mysql-export-allowinsert<br>bin&#x2F;sqoop export <br>–connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root –password hadoop <br>–table allowinsert <br>–export-dir &#x2F;sqoop&#x2F;allowinsert_2&#x2F; <br>–update-key id <br>–update-mode allowinsert<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8796.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8797.png" alt="img"><br>9)查看最终结果<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8798.png" alt="img"><br>10)总结<br>更新导出<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8799.png" alt="img"></p>
<ul>
<li>updateonly  只更新已经存在的数据  不会执行insert增加新的数据</li>
<li>allowinsert  更新已有的数据  插入新的数据 底层相当于insert&amp;update<br>第五步 sqoop job作业介绍<br>1.job语法<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%87100.png" alt="img"><br>2.创建job(–create)<br>在这里，我们创建一个名为myjob，这可以从RDBMS表的数据导入到HDFS作业。下面的命令用于创建一个从DB数据库的employee表导入到HDFS文件的作业。</li>
</ul>
<p>#example13-1-mysql-hdfs-job<br>bin&#x2F;sqoop job –create myjob – import –connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password hadoop <br>–target-dir &#x2F;sqoop&#x2F;sqoopresult555 <br>–table emp –m 1<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%87101.png" alt="img"><br>注意import前要有空格<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%87102.png" alt="img"><br>3.验证job (–list)<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8793.png" alt="img"><br>4.检查job(–show)<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8794.png" alt="img"><br>5.执行job (–exec)<br>‘–exec’ 选项用于执行保存的作业。下面的命令用于执行保存的作业称为myjob。<br>#example13-4-mysql-hdfs-job<br>bin&#x2F;sqoop job –exec myjob<br>sqoop需要输入mysql密码<br>它会显示下面的输出。<br>10&#x2F;08&#x2F;19 13:08:45 INFO tool.CodeGenTool: Beginning code generation<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8795.png" alt="img"><br>6.job的免密输入<br>sqoop在创建job时，使用–password-file参数，可以避免输入mysql密码，如果使用–password将出现警告，并且每次都要手动输入密码才能执行job，sqoop规定密码文件必须存放在HDFS上，并且权限必须是400。<br>echo -n “hadoop” &gt; node1-mysql.pwd<br>hadoop fs -mkdir -p &#x2F;sqoop&#x2F;pwd&#x2F;<br>hadoop fs -put node1-mysql.pwd &#x2F;sqoop&#x2F;pwd&#x2F;<br>hadoop fs -chmod 400 &#x2F;sqoop&#x2F;pwd&#x2F;node1-mysql.pwd<br>检查sqoop的sqoop-site.xml是否存在如下配置：<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8796.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8797.png" alt="img"><br><br>创建sqoop job<br>在创建job时，使用–password-file参数<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8798.png" alt="img"><br>#example14-1-mysql-hdfs-job-nopwd<br>bin&#x2F;sqoop job –create myjob2 – import –connect jdbc:mysql:&#x2F;&#x2F;node1:3306&#x2F;userdb <br>–username root <br>–password-file &#x2F;sqoop&#x2F;pwd&#x2F;node1-mysql.pwd <br>–target-dir &#x2F;sqoop&#x2F;sqoopresult666 <br>–table emp –m 1<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%8799.png" alt="img"><br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%87100.png" alt="img"><br>执行job:<br><img src="/2023/06/19/sqoop/%E5%9B%BE%E7%89%87101.png" alt="img"></p>

  </div>
</article>

    </div>
    <footer class="footer-nav">
      <div class="footer">
        <div class="back-top" id="back-top" title="Back to top">
          <i class="icon icon-chevron-bar-up"></i>
        </div>
        <span class="footer-msg">
  
    <div class="icp">
      <img src="/img/icp.png" alt="icp备案">
      <a href="https://beian.miit.gov.cn/" class="icp-text" target="_blank">
        京ICP备2021005293号
      </a>
    </div>
  

  

  
  

    <div>
      <span id="busuanzi_container_site_pv">
        <span id="busuanzi_value_site_pv">?</span> PV
      </span>
      <span id="busuanzi_container_site_uv">
        <span id="busuanzi_value_site_uv">?</span> UV
      </span>
    </div>

  



  Copyright &copy;
  2020
  
  
    <span class="timeDivide">-</span>
    2023
  
  John Doe.
  Power by
  <a href="https://hexo.io/" target="_blank" rel="external nofollow">Hexo</a>
  and
  <a href="https://github.com/Cerallin/hexo-theme-yuzu"
      target="_blank" rel="external nofollow" title="v2.4">
    Theme Yuzu</a>.
</span>

      </div>
    </footer>
    


<script src="/js/clipboard/clipboard.min.js"></script>


<script src="/js/theme.js"></script>


<script src="/js/index.js"></script>



  
  <script>
      expend = "展开";
      collapse = "收起";
  </script>
  
  
<script src="/js/toc.js"></script>





  </div>
</body>
